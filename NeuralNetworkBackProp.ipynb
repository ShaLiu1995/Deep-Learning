{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Getting started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import MNISTtools\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Read MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 60000)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "xtrain, ltrain = MNISTtools.load(dataset = \"training\", path = \"/datasets/MNIST\")\n",
    "print(xtrain.shape)\n",
    "print(ltrain.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Using MNISTtools.load, store the images and labels from the training datasets into two variables,\n",
    "respectively, xtrain and ltrain. What are the shapes of both variables? What is the size of the\n",
    "training dataset? What is the feature dimension?\n",
    "\n",
    "shape of xtrain: (784, 60000)\n",
    "\n",
    "shape of ltrain: (60000, )\n",
    "\n",
    "size of trainig dataset: 60000\n",
    "\n",
    "feature dimension: 784"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Display the image of index 42 and check that its content corresponds to its label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MNISTtools.show(xtrain[:, 42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) What is the range of xtrain (minimum and maximum values)? What is the type of xtrain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of xtrain is <type 'numpy.ndarray'>\n",
      "The range of xtrain is [0, 255]\n"
     ]
    }
   ],
   "source": [
    "print('The type of xtrain is {}'.format(type(xtrain)))\n",
    "print('The range of xtrain is [{0}, {1}]'.format(xtrain.min(), xtrain.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Create a function that takes a collection of images (such as xtrain) and return a modified version in the range [-1, 1]\n",
    "of type float64. Update xtrain accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_MNIST_images(x):\n",
    "    return np.interp(x, (x.min(), x.max()), (-1, +1)).astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of xtrain is (784, 60000)\n",
      "The range of xtrain is [-1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "xtrain = normalize_MNIST_images(xtrain)\n",
    "print('The shape of xtrain is {}'.format(xtrain.shape))\n",
    "print('The range of xtrain is [{0}, {1}]'.format(xtrain.min(), xtrain.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Using integer array indexing, complete the following function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label2onehot(lbl):\n",
    "    d = np.zeros((lbl.max() + 1, lbl.size))\n",
    "    d[lbl, np.arange(0, lbl.size)] = 1\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of dtrain is (10, 60000)\n",
      "The one-hot code for index 42 is [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "dtrain = label2onehot(ltrain)\n",
    "print('The shape of dtrain is {}'.format(dtrain.shape))\n",
    "print('The one-hot code for index 42 is {}'.format(dtrain[:, 42]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Complete the following function such that ltrain == onehot2label(dtrain)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot2label(d):\n",
    "    lbl = d.argmax(axis=0)\n",
    "    return lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.array_equal(ltrain, onehot2label(dtrain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) Create a function that returns an array whose columns are the 60000 predictions y from an array whose columns are\n",
    "the 60000 vectors a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(a):\n",
    "    m = a.max(axis=0)\n",
    "    temp = np.exp(a - m)\n",
    "    return temp / temp.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) Show that $\\frac{\\partial g(\\boldsymbol{a})}{\\partial a_i} = g(\\boldsymbol{a})_i(1 - g(\\boldsymbol{a})_i)$.\n",
    "\n",
    "$\\frac{\\partial g(\\boldsymbol{a})}{\\partial a_i} = \\frac{exp(a_i)\\sum_{j=1}^{N} (a_j) - exp^2(a_i)}{(\\sum_{j=1}^{N} (a_j))^2} = \\frac{exp(a_i)}{\\sum_{j=1}^{N} (a_j)} \\frac{\\sum_{j=1}^{N} (a_j) - exp(a_i)}{\\sum_{j=1}^{N} (a_j)} = \\frac{exp(a_i)}{\\sum_{j=1}^{N} (a_j)} (1-\\frac{exp(a_i)}{\\sum_{j=1}^{N} (a_j)}) =g(\\boldsymbol{a})_i(1 - g(\\boldsymbol{a})_i) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9) Show that $\\frac{\\partial g(\\boldsymbol{a})}{\\partial a_j} = -g(\\boldsymbol{a})_i g(\\boldsymbol{a})_j$ for $j\\neq i$.\n",
    "\n",
    "$\\frac{\\partial g(\\boldsymbol{a})}{\\partial a_j} = \\frac{0 - exp(a_i)exp(a_j)}{(\\sum_{j=1}^{N} (a_j))^2} = -\\frac{exp(a_i)}{\\sum_{j=1}^{N} (a_j)} \\frac{exp(a_j)}{\\sum_{j=1}^{N} (a_j)} = -g(\\boldsymbol{a})_i g(\\boldsymbol{a})_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10) write a function softmaxp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmaxp(a, e):\n",
    "    delta = np.zeros((a.shape[0], a.shape[1]))\n",
    "    ga = softmax(a)\n",
    "    for k in range(a.shape[1]):\n",
    "        ga_k = ga[:, k]\n",
    "        e_k = e[:, k]\n",
    "        delta[:, k] = np.multiply(ga_k, e_k) - np.dot(ga_k, e_k) * ga_k\n",
    "    return delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11) Complete the following script to check your function softmaxp as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.004681621313714e-07 should be smaller than 1e-6\n"
     ]
    }
   ],
   "source": [
    "eps = 1e-6 # finite difference step\n",
    "a = np.random.randn(10, 200) # random inputs\n",
    "e = np.random.randn(10, 200) # random directions\n",
    "diff = softmaxp(a, e)\n",
    "diff_approx = (softmax(a + eps * e) - softmax(a)) / eps\n",
    "rel_error = np.abs(diff - diff_approx).mean() / np.abs(diff_approx).mean()\n",
    "print(rel_error, 'should be smaller than 1e-6')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12) For the hidden layers, we will be using $ReLU(\\boldsymbol{a})i = max(a_i, 0)$. Write two functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(a):\n",
    "    temp = np.zeros((a.shape[0], a.shape[1]))\n",
    "    for k in range(a.shape[1]):\n",
    "        temp[:, k] = np.maximum(a[:, k], np.zeros(a.shape[0]))\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relup(a, e):\n",
    "    delta = np.zeros((a.shape[0], a.shape[1]))\n",
    "    ga = relu(a)\n",
    "    for k in range(a.shape[1]):\n",
    "        gpa = np.zeros((a.shape[0], a.shape[0]))\n",
    "        for i in range(a.shape[0]):\n",
    "            if ga[i, k] > 0:\n",
    "                gpa[i, i] = 1\n",
    "        delta[:, k] = np.matmul(gpa, e[:, k])\n",
    "    return delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.721347147778074e-11 should be smaller than 1e-6\n"
     ]
    }
   ],
   "source": [
    "eps = 1e-6 # finite difference step\n",
    "a = np.random.randn(10, 200) # random inputs\n",
    "e = np.random.randn(10, 200) # random directions\n",
    "diff = relup(a, e)\n",
    "diff_approx = (relu(a + eps * e) - relu(a)) / eps\n",
    "rel_error = np.abs(diff - diff_approx).mean() / np.abs(diff_approx).mean()\n",
    "print(rel_error, 'should be smaller than 1e-6')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13) Use the following function to create/initialize your shallow network as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_shallow(Ni, Nh, No):\n",
    "    b1 = np.random.randn(Nh, 1) / np.sqrt((Ni+1.)/2.)\n",
    "    W1 = np.random.randn(Nh, Ni) / np.sqrt((Ni+1.)/2.)\n",
    "    b2 = np.random.randn(No, 1) / np.sqrt((Nh+1.))\n",
    "    W2 = np.random.randn(No, Nh) / np.sqrt((Nh+1.))\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ni = xtrain.shape[0]\n",
    "Nh = 64\n",
    "No = dtrain.shape[0]\n",
    "netinit = init_shallow(Ni, Nh, No)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14) Complete the function forwardprop shallow to evaluate the prediction of our initial network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardprop_shallow(x, net):\n",
    "    W1 = net[0]\n",
    "    b1 = net[1]\n",
    "    W2 = net[2]\n",
    "    b2 = net[3]\n",
    "    a1 = W1.dot(x) + b1\n",
    "    h1 = relu(a1)\n",
    "    a2 = W2.dot(h1) + b2\n",
    "    y = softmax(a2)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "yinit = forwardprop_shallow(xtrain, netinit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15) Complete the function eval loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_loss(y, d):\n",
    "    loss = -1.0 * np.sum(d * np.log(y))\n",
    "    return loss / (d.shape[0] * d.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24738259773078927 should be around .26\n"
     ]
    }
   ],
   "source": [
    "print(eval_loss(yinit, dtrain), 'should be around .26')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16) Complete the function eval perfs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_perfs(y, lbl):\n",
    "    return 1.0 * np.sum(np.not_equal(onehot2label(y), lbl)) / y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8573\n"
     ]
    }
   ],
   "source": [
    "print(eval_perfs(yinit, ltrain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17) Complete the following function update shallow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_shallow(x, d, net, gamma=.05):\n",
    "    W1 = net[0]\n",
    "    b1 = net[1]\n",
    "    W2 = net[2]\n",
    "    b2 = net[3]\n",
    "    Ni = W1.shape[1]\n",
    "    Nh = W1.shape[0]\n",
    "    No = W2.shape[0]\n",
    "    gamma = gamma / x.shape[1] # normalized by the training dataset size  \n",
    "    # Forward phase\n",
    "    a1 = W1.dot(x) + b1\n",
    "    h1 = relu(a1)\n",
    "    a2 = W2.dot(h1) + b2\n",
    "    y = softmax(a2)\n",
    "    e = -1.0 * d / y\n",
    "    # Error evaluation\n",
    "    delta2 = softmaxp(a2, e)\n",
    "    delta1 = relup(a1, (W2.T.dot(delta2) + b2.T.dot(delta2)))\n",
    "    # gradient update\n",
    "    W2 = W2 - gamma * delta2.dot(h1.T)\n",
    "    W1 = W1 - gamma * delta1.dot(x.T)\n",
    "    b2 = b2 - gamma * delta2.sum(axis=1).reshape (No, 1)\n",
    "    b1 = b1 - gamma * delta1.sum(axis=1).reshape (Nh, 1)\n",
    "    \n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show that $(\\nabla_y E)_i = -\\frac{di}{yi}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$(\\nabla_y E)_i = \\frac{\\partial(-\\sum_{i=1}^{N} d_i \\log y_i)}{\\partial y_i} = \\frac{0 + \\dotsb + \\partial d_i \\log y_i + \\dotsc + 0}{\\partial y_i} = - \\frac{d_i}{y_i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18) Using update shallow, complete the function backprop shallow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop_shallow(x, d, net, T, gamma=.05):\n",
    "    lbl = onehot2label(d)\n",
    "    for t in range(0, T):\n",
    "        net = update_shallow(x, d, net, gamma)\n",
    "        y = forwardprop_shallow(x, net)\n",
    "        print('iter:', t+1)\n",
    "        print('loss:', eval_loss(y, d))\n",
    "        print('pref:', eval_perfs(y, lbl))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 1\n",
      "loss: 0.22099507165757765\n",
      "pref: 0.795466666667\n",
      "iter: 2\n",
      "loss: 0.21015817470212003\n",
      "pref: 0.741583333333\n",
      "iter: 3\n",
      "loss: 0.202499346740826\n",
      "pref: 0.69365\n",
      "iter: 4\n",
      "loss: 0.19647813281608967\n",
      "pref: 0.615566666667\n",
      "iter: 5\n",
      "loss: 0.19148352419673106\n",
      "pref: 0.61185\n",
      "iter: 6\n",
      "loss: 0.18683624671249616\n",
      "pref: 0.528983333333\n",
      "iter: 7\n",
      "loss: 0.18293182190567744\n",
      "pref: 0.564016666667\n",
      "iter: 8\n",
      "loss: 0.17915014694252585\n",
      "pref: 0.472616666667\n",
      "iter: 9\n",
      "loss: 0.17590771106225384\n",
      "pref: 0.536166666667\n",
      "iter: 10\n",
      "loss: 0.17263940566454006\n",
      "pref: 0.4423\n",
      "iter: 11\n",
      "loss: 0.16951283417931554\n",
      "pref: 0.507333333333\n",
      "iter: 12\n",
      "loss: 0.16593641132780126\n",
      "pref: 0.416383333333\n",
      "iter: 13\n",
      "loss: 0.16285968341744905\n",
      "pref: 0.465483333333\n",
      "iter: 14\n",
      "loss: 0.15848981549602434\n",
      "pref: 0.38505\n",
      "iter: 15\n",
      "loss: 0.15577565052956713\n",
      "pref: 0.4227\n",
      "iter: 16\n",
      "loss: 0.1505532626025688\n",
      "pref: 0.3538\n",
      "iter: 17\n",
      "loss: 0.14855748278594075\n",
      "pref: 0.388366666667\n",
      "iter: 18\n",
      "loss: 0.14284039270922214\n",
      "pref: 0.3297\n",
      "iter: 19\n",
      "loss: 0.14148930012996225\n",
      "pref: 0.361366666667\n",
      "iter: 20\n",
      "loss: 0.1352676067505169\n",
      "pref: 0.30945\n",
      "iter: 21\n",
      "loss: 0.134652378563439\n",
      "pref: 0.33865\n",
      "iter: 22\n",
      "loss: 0.12801030010818146\n",
      "pref: 0.2932\n",
      "iter: 23\n",
      "loss: 0.12819264577124106\n",
      "pref: 0.32295\n",
      "iter: 24\n",
      "loss: 0.12118472548885334\n",
      "pref: 0.279883333333\n",
      "iter: 25\n",
      "loss: 0.12204871817743142\n",
      "pref: 0.30995\n",
      "iter: 26\n",
      "loss: 0.11489886926910219\n",
      "pref: 0.269366666667\n",
      "iter: 27\n",
      "loss: 0.11606461181283846\n",
      "pref: 0.297533333333\n",
      "iter: 28\n",
      "loss: 0.10914942417588924\n",
      "pref: 0.258666666667\n",
      "iter: 29\n",
      "loss: 0.11033848912337096\n",
      "pref: 0.28505\n",
      "iter: 30\n",
      "loss: 0.10394241200421221\n",
      "pref: 0.249316666667\n",
      "iter: 31\n",
      "loss: 0.10501713386578986\n",
      "pref: 0.2725\n",
      "iter: 32\n",
      "loss: 0.09919214036847974\n",
      "pref: 0.240333333333\n",
      "iter: 33\n",
      "loss: 0.1001018895464734\n",
      "pref: 0.26115\n",
      "iter: 34\n",
      "loss: 0.09490511080468408\n",
      "pref: 0.232\n",
      "iter: 35\n",
      "loss: 0.09566267562860498\n",
      "pref: 0.250233333333\n",
      "iter: 36\n",
      "loss: 0.0910306373127382\n",
      "pref: 0.223683333333\n",
      "iter: 37\n",
      "loss: 0.09160149319321248\n",
      "pref: 0.23975\n",
      "iter: 38\n",
      "loss: 0.08755277271168851\n",
      "pref: 0.215816666667\n",
      "iter: 39\n",
      "loss: 0.08795592357532254\n",
      "pref: 0.230333333333\n",
      "iter: 40\n",
      "loss: 0.08444552387013417\n",
      "pref: 0.208966666667\n",
      "iter: 41\n",
      "loss: 0.08469398008951173\n",
      "pref: 0.221133333333\n",
      "iter: 42\n",
      "loss: 0.08164507708638319\n",
      "pref: 0.203966666667\n",
      "iter: 43\n",
      "loss: 0.08176806707062059\n",
      "pref: 0.212983333333\n",
      "iter: 44\n",
      "loss: 0.07910903175355595\n",
      "pref: 0.198883333333\n",
      "iter: 45\n",
      "loss: 0.07913438056630834\n",
      "pref: 0.206566666667\n",
      "iter: 46\n",
      "loss: 0.07681391737485124\n",
      "pref: 0.1944\n",
      "iter: 47\n",
      "loss: 0.07676975959380149\n",
      "pref: 0.200516666667\n",
      "iter: 48\n",
      "loss: 0.07472538392289951\n",
      "pref: 0.1901\n",
      "iter: 49\n",
      "loss: 0.07462593535949391\n",
      "pref: 0.195533333333\n",
      "iter: 50\n",
      "loss: 0.07280148290660755\n",
      "pref: 0.185966666667\n",
      "iter: 51\n",
      "loss: 0.07265917164105337\n",
      "pref: 0.190416666667\n",
      "iter: 52\n",
      "loss: 0.0710215002716843\n",
      "pref: 0.182183333333\n",
      "iter: 53\n",
      "loss: 0.07084698906994645\n",
      "pref: 0.1855\n",
      "iter: 54\n",
      "loss: 0.06937571967511748\n",
      "pref: 0.178616666667\n",
      "iter: 55\n",
      "loss: 0.06917337778118823\n",
      "pref: 0.181633333333\n",
      "iter: 56\n",
      "loss: 0.06784038320912887\n",
      "pref: 0.1747\n",
      "iter: 57\n",
      "loss: 0.06761598422578351\n",
      "pref: 0.1779\n",
      "iter: 58\n",
      "loss: 0.06640291273821829\n",
      "pref: 0.17135\n",
      "iter: 59\n",
      "loss: 0.06616344463655664\n",
      "pref: 0.174833333333\n",
      "iter: 60\n",
      "loss: 0.06506033336700161\n",
      "pref: 0.168333333333\n",
      "iter: 61\n",
      "loss: 0.06480545553843285\n",
      "pref: 0.170983333333\n",
      "iter: 62\n",
      "loss: 0.0638072584848827\n",
      "pref: 0.165366666667\n",
      "iter: 63\n",
      "loss: 0.06354065297436207\n",
      "pref: 0.16785\n",
      "iter: 64\n",
      "loss: 0.06263041082968285\n",
      "pref: 0.162766666667\n",
      "iter: 65\n",
      "loss: 0.06235805937218823\n",
      "pref: 0.164916666667\n",
      "iter: 66\n",
      "loss: 0.06152949215579933\n",
      "pref: 0.160016666667\n",
      "iter: 67\n",
      "loss: 0.06125452201154605\n",
      "pref: 0.162066666667\n",
      "iter: 68\n",
      "loss: 0.06049734338904543\n",
      "pref: 0.157483333333\n",
      "iter: 69\n",
      "loss: 0.060221436830333756\n",
      "pref: 0.1597\n",
      "iter: 70\n",
      "loss: 0.05952865246170958\n",
      "pref: 0.155533333333\n",
      "iter: 71\n",
      "loss: 0.05925349919301498\n",
      "pref: 0.157266666667\n",
      "iter: 72\n",
      "loss: 0.05861851651906413\n",
      "pref: 0.153383333333\n",
      "iter: 73\n",
      "loss: 0.05834378479879368\n",
      "pref: 0.154666666667\n",
      "iter: 74\n",
      "loss: 0.057761753809188535\n",
      "pref: 0.1517\n",
      "iter: 75\n",
      "loss: 0.057489207533871704\n",
      "pref: 0.15285\n",
      "iter: 76\n",
      "loss: 0.05695382666866112\n",
      "pref: 0.150333333333\n",
      "iter: 77\n",
      "loss: 0.05668237103479047\n",
      "pref: 0.151\n",
      "iter: 78\n",
      "loss: 0.056190017377573134\n",
      "pref: 0.148566666667\n",
      "iter: 79\n",
      "loss: 0.05592256392590141\n",
      "pref: 0.149166666667\n",
      "iter: 80\n",
      "loss: 0.05546887687505308\n",
      "pref: 0.147183333333\n",
      "iter: 81\n",
      "loss: 0.05520670541672218\n",
      "pref: 0.147933333333\n",
      "iter: 82\n",
      "loss: 0.054786802067180604\n",
      "pref: 0.14575\n",
      "iter: 83\n",
      "loss: 0.05453003120274372\n",
      "pref: 0.146566666667\n",
      "iter: 84\n",
      "loss: 0.05414114771603317\n",
      "pref: 0.144733333333\n",
      "iter: 85\n",
      "loss: 0.0538896274118178\n",
      "pref: 0.144816666667\n",
      "iter: 86\n",
      "loss: 0.053528076915868035\n",
      "pref: 0.143683333333\n",
      "iter: 87\n",
      "loss: 0.05328231002441038\n",
      "pref: 0.143316666667\n",
      "iter: 88\n",
      "loss: 0.052945184481896314\n",
      "pref: 0.1423\n",
      "iter: 89\n",
      "loss: 0.05270494779371481\n",
      "pref: 0.14195\n",
      "iter: 90\n",
      "loss: 0.052390116595954106\n",
      "pref: 0.14105\n",
      "iter: 91\n",
      "loss: 0.05215634636333553\n",
      "pref: 0.140816666667\n",
      "iter: 92\n",
      "loss: 0.05186112258000702\n",
      "pref: 0.1399\n",
      "iter: 93\n",
      "loss: 0.05163315765317617\n",
      "pref: 0.139633333333\n",
      "iter: 94\n",
      "loss: 0.05135561257700486\n",
      "pref: 0.1388\n",
      "iter: 95\n",
      "loss: 0.05113373544409303\n",
      "pref: 0.1384\n",
      "iter: 96\n",
      "loss: 0.05087212411836976\n",
      "pref: 0.13775\n",
      "iter: 97\n",
      "loss: 0.050656322567388765\n",
      "pref: 0.137533333333\n",
      "iter: 98\n",
      "loss: 0.050408828790915516\n",
      "pref: 0.1366\n",
      "iter: 99\n",
      "loss: 0.05019906750515616\n",
      "pref: 0.1363\n",
      "iter: 100\n",
      "loss: 0.049964273705428684\n",
      "pref: 0.135583333333\n"
     ]
    }
   ],
   "source": [
    "nettrain = backprop_shallow(xtrain, dtrain, netinit, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "19) Load the testing dataset into two variables xtest and ltest. What is the size of the testing set?\n",
    "Evaluate the performance of your network on the testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 10000)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "xtest, ltest = MNISTtools.load(dataset = \"testing\", path = \"/datasets/MNIST\")\n",
    "print(xtest.shape)\n",
    "print(ltest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on the testing dataset: 0.1259\n"
     ]
    }
   ],
   "source": [
    "xtest = normalize_MNIST_images(xtest)\n",
    "ytest = forwardprop_shallow(xtest, nettrain)\n",
    "print('Performance on the testing dataset:', eval_perfs(ytest, ltest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20) Using update shallow, complete and run for 5 epochs the function backprop minibatch shallow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop_minibatch_shallow(x, d, net, T, B=100, gamma=.05):\n",
    "    N = x.shape[1]\n",
    "    lbl = onehot2label(d)\n",
    "    for t in range(0, T):\n",
    "        for l in range(0, (N+B-1)/B):\n",
    "            idx = np.arange(B*l, min(B*(l+1), N))\n",
    "            net = update_shallow(x[:, idx], d[:, idx], net, gamma)\n",
    "        y = forwardprop_shallow(x, net)\n",
    "        print('iter:', t+1)\n",
    "        print('loss:', eval_loss(y, d))\n",
    "        print('pref:', eval_perfs(y, lbl))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 1\n",
      "loss: 0.03328349740419724\n",
      "pref: 0.103483333333\n",
      "iter: 2\n",
      "loss: 0.026496658023473663\n",
      "pref: 0.0818333333333\n",
      "iter: 3\n",
      "loss: 0.0220861297376514\n",
      "pref: 0.0672666666667\n",
      "iter: 4\n",
      "loss: 0.019136917544357267\n",
      "pref: 0.05785\n",
      "iter: 5\n",
      "loss: 0.017160353810890448\n",
      "pref: 0.0517833333333\n"
     ]
    }
   ],
   "source": [
    "netminibatch = backprop_minibatch_shallow(xtrain, dtrain, netinit, 5, B=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "21) Compare the performance of this new network on the testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance(minbatch) on the testing dataset: 0.0566\n"
     ]
    }
   ],
   "source": [
    "ytest = forwardprop_shallow(xtest, netminibatch)\n",
    "print('Performance on the testing dataset:', eval_perfs(ytest, ltest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Experiment with network topology and learning parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_process(Nh=64, gamma=.05, B=100, T=5):\n",
    "    netinit = init_shallow(Ni, Nh, No)\n",
    "    netminibatch = backprop_minibatch_shallow(xtrain, dtrain, netinit, T, B=B, gamma=gamma)\n",
    "    ytest = forwardprop_shallow(xtest, netminibatch)\n",
    "    print('Performance on the testing dataset:', eval_perfs(ytest, ltest))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "22) Try with Nh = 16 and Nh = 256 number of hidden units. Look at the training and testing errors.\n",
    "Interpret the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 1\n",
      "loss: 0.03784289534260053\n",
      "pref: 0.1144\n",
      "iter: 2\n",
      "loss: 0.03249615714951529\n",
      "pref: 0.09765\n",
      "iter: 3\n",
      "loss: 0.030372753701103303\n",
      "pref: 0.0897666666667\n",
      "iter: 4\n",
      "loss: 0.02722050321314488\n",
      "pref: 0.0801833333333\n",
      "iter: 5\n",
      "loss: 0.025548431656543517\n",
      "pref: 0.0748333333333\n",
      "Performance on the testing dataset: 0.074\n"
     ]
    }
   ],
   "source": [
    "main_process(Nh=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 1\n",
      "loss: 0.02968378244923598\n",
      "pref: 0.0923666666667\n",
      "iter: 2\n",
      "loss: 0.022057718541865903\n",
      "pref: 0.0671\n",
      "iter: 3\n",
      "loss: 0.01777024450980008\n",
      "pref: 0.0526333333333\n",
      "iter: 4\n",
      "loss: 0.015067590854054649\n",
      "pref: 0.0444833333333\n",
      "iter: 5\n",
      "loss: 0.013057687565317458\n",
      "pref: 0.0382166666667\n",
      "Performance on the testing dataset: 0.0422\n"
     ]
    }
   ],
   "source": [
    "main_process(Nh=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "23) Try different step sizes: $\\gamma=.02$ and $\\gamma=.08$ . Look at the training and testing errors. Interpret the\n",
    "results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 1\n",
      "loss: 0.0377927403561074\n",
      "pref: 0.110566666667\n",
      "iter: 2\n",
      "loss: 0.03053799921507424\n",
      "pref: 0.0899333333333\n",
      "iter: 3\n",
      "loss: 0.026793430927492243\n",
      "pref: 0.0783833333333\n",
      "iter: 4\n",
      "loss: 0.0241458492991156\n",
      "pref: 0.0695\n",
      "iter: 5\n",
      "loss: 0.02216824749621067\n",
      "pref: 0.0637333333333\n",
      "Performance on the testing dataset: 0.0655\n"
     ]
    }
   ],
   "source": [
    "main_process(gamma=.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 1\n",
      "loss: 0.032558241669444606\n",
      "pref: 0.102066666667\n",
      "iter: 2\n",
      "loss: 0.025899467975362214\n",
      "pref: 0.0821\n",
      "iter: 3\n",
      "loss: 0.021378142912993923\n",
      "pref: 0.0672\n",
      "iter: 4\n",
      "loss: 0.01856633824397229\n",
      "pref: 0.0589333333333\n",
      "iter: 5\n",
      "loss: 0.0168440556233523\n",
      "pref: 0.0530666666667\n",
      "Performance on the testing dataset: 0.0558\n"
     ]
    }
   ],
   "source": [
    "main_process(gamma=.08)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "24) Try with minibatches of sizes: B = 50 and B = 200. Look at the training and testing errors. Interpret the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 1\n",
      "loss: 0.026457507746656492\n",
      "pref: 0.0811166666667\n",
      "iter: 2\n",
      "loss: 0.019876275570738993\n",
      "pref: 0.0611\n",
      "iter: 3\n",
      "loss: 0.016793942688778506\n",
      "pref: 0.0512\n",
      "iter: 4\n",
      "loss: 0.01477494319205105\n",
      "pref: 0.0453833333333\n",
      "iter: 5\n",
      "loss: 0.013236229979162309\n",
      "pref: 0.0408666666667\n",
      "Performance on the testing dataset: 0.0453\n"
     ]
    }
   ],
   "source": [
    "main_process(B=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 1\n",
      "loss: 0.03663839503476053\n",
      "pref: 0.113233333333\n",
      "iter: 2\n",
      "loss: 0.028753850816845332\n",
      "pref: 0.08815\n",
      "iter: 3\n",
      "loss: 0.024749061577572624\n",
      "pref: 0.075\n",
      "iter: 4\n",
      "loss: 0.02212076557970242\n",
      "pref: 0.0666333333333\n",
      "iter: 5\n",
      "loss: 0.02014415454182866\n",
      "pref: 0.0602166666667\n",
      "Performance on the testing dataset: 0.0604\n"
     ]
    }
   ],
   "source": [
    "main_process(B=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "25) Try minibatch gradient descent with more epochs. What is the best testing error that you can achieved?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 1\n",
      "loss: 0.03279011023127361\n",
      "pref: 0.10245\n",
      "iter: 2\n",
      "loss: 0.025253698887876843\n",
      "pref: 0.0772333333333\n",
      "iter: 3\n",
      "loss: 0.02095312121912967\n",
      "pref: 0.0644333333333\n",
      "iter: 4\n",
      "loss: 0.0181216352157772\n",
      "pref: 0.0551666666667\n",
      "iter: 5\n",
      "loss: 0.0160197126653291\n",
      "pref: 0.0484333333333\n",
      "iter: 6\n",
      "loss: 0.014499008991198183\n",
      "pref: 0.0437166666667\n",
      "iter: 7\n",
      "loss: 0.013256992446599546\n",
      "pref: 0.0399333333333\n",
      "iter: 8\n",
      "loss: 0.012298439883220462\n",
      "pref: 0.0368166666667\n",
      "iter: 9\n",
      "loss: 0.011534316708511096\n",
      "pref: 0.0344833333333\n",
      "iter: 10\n",
      "loss: 0.010877319074970013\n",
      "pref: 0.0325666666667\n",
      "Performance on the testing dataset: 0.0384\n"
     ]
    }
   ],
   "source": [
    "main_process(T=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 1\n",
      "loss: 0.03355739450160582\n",
      "pref: 0.10205\n",
      "iter: 2\n",
      "loss: 0.027699477425903122\n",
      "pref: 0.0844\n",
      "iter: 3\n",
      "loss: 0.024069476367092082\n",
      "pref: 0.0739\n",
      "iter: 4\n",
      "loss: 0.02112860990454647\n",
      "pref: 0.0643666666667\n",
      "iter: 5\n",
      "loss: 0.01882449553714287\n",
      "pref: 0.0579\n",
      "iter: 6\n",
      "loss: 0.016676869015463607\n",
      "pref: 0.0512666666667\n",
      "iter: 7\n",
      "loss: 0.015197921176321972\n",
      "pref: 0.04605\n",
      "iter: 8\n",
      "loss: 0.013946544941003574\n",
      "pref: 0.0422333333333\n",
      "iter: 9\n",
      "loss: 0.012779966543595544\n",
      "pref: 0.0382833333333\n",
      "iter: 10\n",
      "loss: 0.011887868043128491\n",
      "pref: 0.0355833333333\n",
      "iter: 11\n",
      "loss: 0.011131799609425064\n",
      "pref: 0.0332\n",
      "iter: 12\n",
      "loss: 0.01043995114239194\n",
      "pref: 0.0311166666667\n",
      "iter: 13\n",
      "loss: 0.009819531512687338\n",
      "pref: 0.02925\n",
      "iter: 14\n",
      "loss: 0.009273518282679968\n",
      "pref: 0.0276166666667\n",
      "iter: 15\n",
      "loss: 0.008832588833555708\n",
      "pref: 0.0262666666667\n",
      "iter: 16\n",
      "loss: 0.008412561472307421\n",
      "pref: 0.0248333333333\n",
      "iter: 17\n",
      "loss: 0.008031627153372805\n",
      "pref: 0.0239\n",
      "iter: 18\n",
      "loss: 0.007691058832576363\n",
      "pref: 0.0227666666667\n",
      "iter: 19\n",
      "loss: 0.007377663328841161\n",
      "pref: 0.0221666666667\n",
      "iter: 20\n",
      "loss: 0.007074287116557084\n",
      "pref: 0.0212\n",
      "Performance on the testing dataset: 0.0338\n"
     ]
    }
   ],
   "source": [
    "main_process(T=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best performance is around 0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
